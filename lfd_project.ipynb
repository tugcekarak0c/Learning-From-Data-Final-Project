{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa084d-0bfe-4a0d-8be4-e883e9033e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-api-python-client pandas tqdm nltk gensim scikit-learn matplotlib seaborn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "API_KEY = \"AIzaSyDJAmeSDCsRXKUz3JWuZ0Z4hRZeEdF88Dg\" \n",
    "\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "# --- FUNCTIONS ---\n",
    "def fetch_comments(video_id: str, max_comments: int = 300, sleep_s: float = 0.2):\n",
    "    \"\"\"\n",
    "    FETCHES COMMENTS\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    page_token = None\n",
    "\n",
    "    while len(out) < max_comments:\n",
    "        try:\n",
    "            req = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                pageToken=page_token,\n",
    "                textFormat=\"plainText\",\n",
    "                order=\"relevance\"\n",
    "            )\n",
    "            res = req.execute()\n",
    "\n",
    "            items = res.get(\"items\", [])\n",
    "            if not items:\n",
    "                break\n",
    "\n",
    "            for it in items:\n",
    "                top = it[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "                text = (top.get(\"textDisplay\") or \"\").strip()\n",
    "                like = top.get(\"likeCount\", 0)\n",
    "                if text:\n",
    "                    out.append({\"video_id\": video_id, \"text\": text, \"likeCount\": like})\n",
    "                if len(out) >= max_comments:\n",
    "                    break\n",
    "\n",
    "            page_token = res.get(\"nextPageToken\")\n",
    "            if not page_token:\n",
    "                break\n",
    "            \n",
    "            time.sleep(sleep_s)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"error: ({video_id}): {e}\")\n",
    "            break\n",
    "\n",
    "    return out\n",
    "\n",
    "# --- VÄ°DEO LÄ°ST ---\n",
    "topics = {\n",
    "    \"vlog\": [\"-6rhSam8th8\", \"bzE-IMaegzQ\", \"kkEmmo1Bi9c\", \"yjemOKmSkh0\", \"nJUiCWr-2aI\", \"Xzo5witfaW0\"],\n",
    "    \"music\": [\"pFptt7Cargc\", \"a5uQMwRMHcs\", \"C-u5WLJ9Yk4\", \"iS1g8G_njx8\", \"DUT5rEU6pqM\"],\n",
    "    \"game\": [\"t7tKhyjea1w\", \"L8B-kAmlUVk\", \"-BRSus9eU10\", \"O9wsxkRANCs\", \"ss16ENYNJIU\"],\n",
    "    \"comedy\": [\"Tn7g2cn7iBo\", \"VyEINfRMvdc\", \"8Q1z0CMjKXc\", \"yzh7RtIJKZk\", \"57pGarTBJrU\", \"3SB8pSFL0Vo\"],\n",
    "    \"news\": [\"NZF5GDqq0Ro\", \"airkjnJ7dDs\", \"281JfY7HLB4\", \"bMEeK8JdceI\", \"5tOU-kZsmGE\", \"gkjW9PZBRfk\"],\n",
    "    \"food\": [\"QMYOEG4yGE4\", \"HSeohGLqhQs\", \"qEowX-vOb4E\", \"AJbJYwEduso\", \"RaLzxZryEoA\", \"EaljSnLrJW8\"]\n",
    "}\n",
    "\n",
    "label_map = {topic: i for i, topic in enumerate(topics.keys())}\n",
    "\n",
    "# --- COLLECT DATA LOOP ---\n",
    "def collect_all_data(topics_dict, max_comments=300):\n",
    "    rows = []\n",
    "    for topic, video_ids in topics_dict.items():\n",
    "        label = label_map[topic]\n",
    "        print(f\"--> 'The category '{topic}' is being processed....\")\n",
    "        \n",
    "        for vid in tqdm(video_ids):\n",
    "            comments = fetch_comments(vid, max_comments=max_comments)\n",
    "            for c in comments:\n",
    "                rows.append({\n",
    "                    \"topic\": topic,\n",
    "                    \"label\": label,\n",
    "                    \"video_id\": vid,\n",
    "                    \"raw_text\": c[\"text\"], # DIRTY DATA\n",
    "                    \"likeCount\": c[\"likeCount\"]\n",
    "                })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df = collect_all_data(topics, max_comments=300)\n",
    "print(f\"\\nTOTAL NUMBER OF DATAS: {len(df)}\")\n",
    "display(df.head())\n",
    "display(df[\"topic\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6544bfd4-08cf-4e05-8e7b-1d5a75bdb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- PREPROCESSING ---\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def advanced_clean(text):\n",
    "    # 1. LOWER CASE\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # 2. REMOVE LINKS AND USER TAGS\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # 3. REMOVE EVERYTHING EXCEPT LETTERS\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # 4. Tokenization \n",
    "    tokens = text.split()\n",
    "    \n",
    "    # 5. Stop words removal and Lemmatization \n",
    "    filtered_tokens = [\n",
    "        lemmatizer.lemmatize(token) \n",
    "        for token in tokens \n",
    "        if token not in stop_words and len(token) > 2\n",
    "    ]\n",
    "    \n",
    "    # Reconstruct sentences\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# Cleaning process\n",
    "tqdm.pandas(desc=\"Cleaning\")\n",
    "df['clean_text'] = df['raw_text'].progress_apply(advanced_clean)\n",
    "\n",
    "# Drop empty rows (rows containing only emojis or links might have become empty)\n",
    "df = df[df['clean_text'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Data count after cleaning: {len(df)}\")\n",
    "\n",
    "# Example: Raw vs Clean version (Great table for the report)\n",
    "print(\"\\n--- Preprocessing Example ---\")\n",
    "print(df[['raw_text', 'clean_text']].sample(3).to_markdown())\n",
    "\n",
    "# --- STEP 3: DATA SPLITTING (TRAIN / VAL / TEST) ---\n",
    "# Project requirements: Train/Val/Test split and Class Imbalance control \n",
    "\n",
    "X = df['clean_text']\n",
    "y = df['label'] # Numerical labels (0, 1, 2...)\n",
    "\n",
    "# First, split into Train (70%) and Temporary (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Split the remaining into Validation (15%) and Test (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset Distribution:\")\n",
    "print(f\"Training Set   : {len(X_train)} ({len(X_train)/len(df):.0%})\")\n",
    "print(f\"Validation Set : {len(X_val)} ({len(X_val)/len(df):.0%})\")\n",
    "print(f\"Test Set       : {len(X_test)} ({len(X_test)/len(df):.0%})\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65590580-7128-4712-b193-06096699c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# --- STEP 3: FEATURE ENGINEERING ---\n",
    "\n",
    "# 1. TF-IDF Vectorization (Classical Method)\n",
    "print(\"Creating TF-IDF matrices...\")\n",
    "# Top 5000 words/bigrams\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2)) \n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# 2. Word2Vec (Modern Method - Project Requirement)\n",
    "# Training word vectors with our own data\n",
    "print(\"Training Word2Vec model...\")\n",
    "\n",
    "# Convert sentences to token lists (Tokenize)\n",
    "train_tokens = [text.split() for text in X_train]\n",
    "\n",
    "# Build Word2Vec model (100-dimensional vector for each word)\n",
    "w2v_model = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=2, workers=4)\n",
    "\n",
    "def get_mean_vector(text, model):\n",
    "    \"\"\"Calculates the mean of word vectors in a sentence.\"\"\"\n",
    "    tokens = text.split()\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(100) # Return zero vector if no words present\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Convert all data to vectors\n",
    "X_train_w2v = np.array([get_mean_vector(text, w2v_model) for text in X_train])\n",
    "X_val_w2v = np.array([get_mean_vector(text, w2v_model) for text in X_val])\n",
    "X_test_w2v = np.array([get_mean_vector(text, w2v_model) for text in X_test])\n",
    "\n",
    "# 3. Custom Feature (Comment Length)\n",
    "# Comment length can sometimes hint at the topic (e.g., Political comments might be long, music short)\n",
    "def get_length_feature(text_series):\n",
    "    return text_series.apply(lambda x: len(x.split())).values.reshape(-1, 1)\n",
    "\n",
    "X_train_len = get_length_feature(X_train)\n",
    "X_val_len = get_length_feature(X_val)\n",
    "X_test_len = get_length_feature(X_test)\n",
    "\n",
    "# Add custom feature to TF-IDF (Hybrid Feature)\n",
    "X_train_hybrid = hstack([X_train_tfidf, X_train_len])\n",
    "X_val_hybrid = hstack([X_val_tfidf, X_val_len])\n",
    "X_test_hybrid = hstack([X_test_tfidf, X_test_len])\n",
    "\n",
    "print(\"\\n--- Feature Shapes ---\")\n",
    "print(f\"TF-IDF Shape  : {X_train_tfidf.shape}\")\n",
    "print(f\"Word2Vec Shape: {X_train_w2v.shape} (100-dim vector per comment)\")\n",
    "print(f\"Hybrid Shape  : {X_train_hybrid.shape} (TF-IDF + Length)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d97dae-c32e-40a0-9658-a53f65378e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import time\n",
    "\n",
    "# --- STEP 4: MODEL TRAINING AND TUNING ---\n",
    "\n",
    "results = {} # Dictionary to store results\n",
    "\n",
    "print(\"1. MODEL: Logistic Regression (Tuning with Grid Search)...\")\n",
    "# Project Requirement: Must demonstrate Hyperparameter Tuning\n",
    "lr_params = {\n",
    "    'C': [0.1, 1, 10], \n",
    "    'solver': ['liblinear'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "# Searching for best parameters with 5-Fold Cross Validation\n",
    "grid_lr = GridSearchCV(LogisticRegression(), lr_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_lr.fit(X_train_hybrid, y_train)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "best_lr = grid_lr.best_estimator_\n",
    "print(f\"   -> Best parameters: {grid_lr.best_params_}\")\n",
    "print(f\"   -> Training Time: {train_time:.2f} seconds\")\n",
    "print(f\"   -> Best CV Score: {grid_lr.best_score_:.4f}\")\n",
    "\n",
    "results['LogReg'] = {\n",
    "    'model': best_lr,\n",
    "    'features_train': X_train_hybrid,\n",
    "    'features_test': X_test_hybrid,\n",
    "    'time': train_time\n",
    "}\n",
    "\n",
    "print(\"\\n2. MODEL: Linear SVM (Category A - Linear Model)...\")\n",
    "start_time = time.time()\n",
    "svm_model = LinearSVC(C=1.0, max_iter=2000, dual=False) # dual=False for speed preference\n",
    "svm_model.fit(X_train_hybrid, y_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"   -> Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "results['SVM'] = {\n",
    "    'model': svm_model,\n",
    "    'features_train': X_train_hybrid,\n",
    "    'features_test': X_test_hybrid,\n",
    "    'time': train_time\n",
    "}\n",
    "\n",
    "print(\"\\n3. MODEL: Random Forest (Category A - Non-linear)...\")\n",
    "start_time = time.time()\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_hybrid, y_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"   -> Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "results['RandomForest'] = {\n",
    "    'model': rf_model,\n",
    "    'features_train': X_train_hybrid,\n",
    "    'features_test': X_test_hybrid,\n",
    "    'time': train_time\n",
    "}\n",
    "\n",
    "print(\"\\n4. MODEL: MLP Classifier (Category B - Deep Learning)...\")\n",
    "# Deep Learning models generally work better with Dense vectors (Word2Vec).\n",
    "# Therefore, we provide Word2Vec to MLP instead of TF-IDF to ensure diversity.\n",
    "start_time = time.time()\n",
    "# Preventing overfitting with Early stopping (Project Requirement)\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=50, early_stopping=True, random_state=42)\n",
    "mlp_model.fit(X_train_w2v, y_train)\n",
    "train_time = time.time() - start_time\n",
    "print(f\"   -> Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "results['MLP (Deep Learning)'] = {\n",
    "    'model': mlp_model,\n",
    "    'features_train': X_train_w2v,\n",
    "    'features_test': X_test_w2v, # We will use W2V for testing as well\n",
    "    'time': train_time\n",
    "}\n",
    "\n",
    "print(\"\\n--- ALL MODELS TRAINED ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c3052d-901b-44bf-96cf-23970ca805a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "# --- STEP 5: ANALYSIS AND VISUALIZATION ---\n",
    "\n",
    "final_comparison = []\n",
    "\n",
    "print(\"Evaluating models on the Test Set...\")\n",
    "\n",
    "for model_name, data in results.items():\n",
    "    model = data['model']\n",
    "    X_test_feat = data['features_test']\n",
    "    \n",
    "    # Make prediction\n",
    "    y_pred = model.predict(X_test_feat)\n",
    "    \n",
    "    # Calculate scores\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted') # Weighted average for potential class imbalance\n",
    "    \n",
    "    final_comparison.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': acc,\n",
    "        'F1 Score': f1,\n",
    "        'Training Time (s)': data['time']\n",
    "    })\n",
    "\n",
    "# 1. COMPARISON TABLE\n",
    "df_results = pd.DataFrame(final_comparison).sort_values(by='Accuracy', ascending=False)\n",
    "print(\"\\n--- MODEL PERFORMANCE TABLE ---\")\n",
    "print(df_results.to_markdown(index=False))\n",
    "\n",
    "# Select the best model\n",
    "best_model_name = df_results.iloc[0]['Model']\n",
    "best_model_data = results[best_model_name]\n",
    "print(f\"\\nðŸ† BEST MODEL: {best_model_name}\")\n",
    "\n",
    "# 2. CONFUSION MATRIX (For the best model)\n",
    "y_pred_best = best_model_data['model'].predict(best_model_data['features_test'])\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=topics.keys(), yticklabels=topics.keys())\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# 3. LEARNING CURVE (MANDATORY for Bias-Variance Analysis)\n",
    "# Plotting the learning process of the best model\n",
    "print(f\"\\nðŸ“Š Drawing Learning Curve ({best_model_name})... (This may take a while)\")\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    best_model_data['model'], \n",
    "    best_model_data['features_train'], \n",
    "    y_train, \n",
    "    cv=5, \n",
    "    n_jobs=-1, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 5)\n",
    ")\n",
    "\n",
    "# Calculate means\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "plt.plot(train_sizes, val_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "plt.title(f\"Learning Curve - {best_model_name}\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef5ee73-d4f1-4ef0-8cb5-242229a26fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# --- STEP 6: VISUALIZING MODEL PREDICTIONS ---\n",
    "\n",
    "# 1. Get predictions again using the best model\n",
    "best_model = results['SVM']['model']\n",
    "X_test_transformed = results['SVM']['features_test']\n",
    "predictions = best_model.predict(X_test_transformed)\n",
    "\n",
    "# 2. Map numerical labels back to category names (0 -> vlog, 1 -> music...)\n",
    "# Inverting the label_map dictionary\n",
    "inv_label_map = {v: k for k, v in label_map.items()}\n",
    "\n",
    "# 3. Create a comparison DataFrame\n",
    "# Reset indices to avoid misalignment\n",
    "X_test_reset = X_test.reset_index(drop=True)\n",
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Cleaned Text': X_test_reset,\n",
    "    'True Category': y_test_reset.map(inv_label_map),\n",
    "    'Model Prediction': [inv_label_map[p] for p in predictions]\n",
    "})\n",
    "\n",
    "# 4. Show 5 Correct Predictions\n",
    "print(\"\\nâœ… CORRECT PREDICTIONS:\")\n",
    "print(comparison_df[comparison_df['True Category'] == comparison_df['Model Prediction']].sample(5)[['Cleaned Text', 'True Category', 'Model Prediction']].to_markdown(index=False))\n",
    "\n",
    "# 5. Show 5 Incorrect Predictions (Great for Error Analysis!)\n",
    "print(\"\\nâŒ INCORRECT PREDICTIONS (Error Analysis):\")\n",
    "wrong_preds = comparison_df[comparison_df['True Category'] != comparison_df['Model Prediction']]\n",
    "\n",
    "if len(wrong_preds) > 0:\n",
    "    print(wrong_preds.sample(min(5, len(wrong_preds)))[['Cleaned Text', 'True Category', 'Model Prediction']].to_markdown(index=False))\n",
    "else:\n",
    "    print(\"The model made no mistakes! (Highly unlikely but possible)\")\n",
    "\n",
    "# 6. Live Test: Testing with a custom sentence\n",
    "print(\"\\n--- LIVE TEST (CUSTOM INPUT) ---\")\n",
    "my_comment = [\"this game has amazing graphics but the story is boring\"]\n",
    "\n",
    "# Preprocess the custom input exactly like the training data\n",
    "my_comment_clean = [advanced_clean(c) for c in my_comment]\n",
    "\n",
    "# Extract features (TF-IDF + Length)\n",
    "my_tfidf = tfidf.transform(my_comment_clean)\n",
    "my_len = get_length_feature(pd.Series(my_comment_clean))\n",
    "\n",
    "my_features = hstack([my_tfidf, my_len])\n",
    "\n",
    "# Predict\n",
    "my_pred_index = best_model.predict(my_features)[0]\n",
    "print(f\"Comment: '{my_comment[0]}'\")\n",
    "print(f\"Model Prediction: ðŸŽ¯ {inv_label_map[my_pred_index].upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e0d98-149e-4333-ab9b-39242e2edfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
